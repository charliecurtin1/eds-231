---
title: "Lab5"
author: "Charlie Curtin"
date: "2024-05-08"
output: html_document
---

```{r, include = FALSE, message = FALSE}
library(tidytext)
library(tidyverse)
library(widyr)
library(irlba)
library(broom) 
library(textdata)
library(readr)
library(LexisNexisTools)
library(stringr)
library(here)
```

## Lab 5 Assignment

### Train Your Own Embeddings

1.  Using the data from your Nexis Uni query from Week 2, create a set of word embeddings. To do this, you'll essentially need to recreate the steps in today's example down through the chunk named "pmi". 

2.  Think of 3 important words in your data set. Calculate and plot the 10 most semantically similar words for each of them. Identify and interpret any interesting or surprising results.

3.  Assemble 3 word math equations that you think could be useful or interesting for exploring the meaning of key words or ideas in your data set.

My keywords from my Nexis search were "soil health".
```{r}
# list files downloaded from Nexis
post_files <- list.files(pattern = ".docx", path = here("data/Lab2/"),
                      full.names = TRUE, 
                      recursive = TRUE, 
                      ignore.case = TRUE)

# read in files
dat <- lnt_read(post_files, convert_date = FALSE, remove_cover = FALSE)

# extract relevant outputs from the LNT output
meta_df <- dat@meta
articles_df <- dat@articles
paragraphs_df <- dat@paragraphs

# convert date, headline, and article text into a tibble
dat2 <- tibble(Date = meta_df$Date,
               Headline = meta_df$Headline,
               ID = articles_df$ID,
               text = articles_df$Article)

# clean data
dat2 <- dat2 %>% 
  # remove items that aren't articles
  filter(!is.na(Date)) %>% 
  # remove duplicate articles
  filter(!ID %in% c(35, 41, 88, 89))

# load stop words
data("stop_words")
```

Calculate unigram probabilities, normalized skipgram probabilities, and the point-wise mutual information measure
```{r}
# calculate unigram probabilities
unigram_probs <- dat2 %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word") %>% 
  # count frequencies of words
  count(word, sort = TRUE) %>% 
  # convert that to probability of occurrence (frequency percentage)
  mutate(prob = n/sum(n))

# extract sliding window skipgram tokens
skipgrams <- dat2 %>% 
  # break down text into 5 word segments
  unnest_tokens(ngram, text, token = "ngrams", n = 5) %>% 
  mutate(ngramID = row_number()) %>% 
  # create a new column by uniting these two columns
  tidyr::unite(col = skipgramID, ID, ngramID) %>% 
  unnest_tokens(word, ngram) %>% 
  anti_join(stop_words, by = "word")

# calculate skipgram probabilities
skipgram_probs  <- skipgrams %>%
  pairwise_count(item = word, feature = skipgramID, upper = FALSE) %>% 
  mutate(prob = n/sum(n))

# normalize skipgram probabilities
normalized_probs <- skipgram_probs %>% 
  rename(word1 = item1, word2 = item2) %>% 
  left_join(unigram_probs %>% 
              select(word1 = word, prob1 = prob), by = "word1") %>% 
  left_join(unigram_probs %>% 
              select(word2 = word, prob2 = prob), by = "word2") %>% 
  mutate(prob_together = prob/prob1/prob2)

# calculate point-wise mutual information measure in a matrix
pmi_matrix <- normalized_probs %>% 
  mutate(pmi = log10(prob_together)) %>% 
  cast_sparse(word1, word2, pmi)
```

Decompose the pmi matrix into fewer dimensions and create a function that calculates similarities between words
```{r}
# references all of the elements of our matrix (@x), and if there's any NAs, replace them with 0
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

# decompose our matrix into fewer dimensions
pmi_svd <- irlba(pmi_matrix, 100, verbose = FALSE)

# extract word vectors
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)

# write function that calculate similarities between words to find synonyms
search_synonyms <- function(word_vectors, selected_vector, original_word) {
  
  # dot product of matrix and our single vector
  dat <- word_vectors %*% selected_vector
  
  # calculate similarities
  similarities <- as.data.frame(dat) %>% 
    tibble(token = rownames(dat), similarity = dat[,1]) %>% 
    filter(token != original_word) %>% 
    arrange(desc(similarity)) %>% 
    select(token, similarity)
  
  return(similarities)
}
```

Find synonyms to "soil", "crops", and "management" using the function
```{r}
# soil synonyms
soil_syn <- search_synonyms(word_vectors, word_vectors["soil",], "soil")

# crops synonyms
crops_syn <- search_synonyms(word_vectors, word_vectors["crops",], "crops")

# management synonyms
management_syn <- search_synonyms(word_vectors, word_vectors["management",], "management")
```

Plot 10 most similar words to the target words
```{r}
# row bind the synonym dataframes together
soil_syn %>% 
  mutate(selected = "soil") %>% 
  bind_rows(crops_syn %>% 
              mutate(selected = "crops"),
            management_syn %>% 
              mutate(selected = "management")) %>% 
  # group by the selected word and find top 10 most similar words
  group_by(selected) %>% 
  top_n(10, similarity) %>% 
  # reorder by similarity score
  mutate(token = reorder(token, similarity)) %>% 
  # plot top 10 most similar words to each target word
  ggplot(aes(reorder(token, similarity), similarity, fill = selected)) +
  geom_col(show.legend = FALSE) +
  theme_bw() +
  coord_flip() +
  facet_wrap(~selected, scales = "free") +
  labs(x = NULL, title = "Word vectors most similar to crops, management, and soil")
```
Most of the most similar words make sense for each of our target words. "Cover crops" is a specific phrase, and some of the other words similar to "crops" are actual crops, like "soybeans", "alfalfa", and "wheat". A lot of the words for management make sense when paired with the target word, like "practices", "waste", and "nutrient". I'm not sure how the word "inaugural" is included, but I believe "icar" refers to the acronym ICAR, or the Indian Council of Agricultural Research. A few of the articles in the corpus referred to a specific policy about soil management in India. For "soil", "health" as the most similar word is obvious, as those two words were my initial search term in Nexis.

Word math equations
```{r}
# removing health from soil
soil_no_health <- word_vectors["soil",] - word_vectors["health",]

# adding soil and health
soil_health <- word_vectors["soil",] + word_vectors["health",]

# removing specific crops from crops
crops_no_crops <- word_vectors["crops",] - (word_vectors["soybeans",] + word_vectors["wheat",] + word_vectors["alfalfa",])
```


#### Pretrained Embeddings

4.  Following the example in the SMLTR text (section 5.4), create a set of 100-dimensional GloVe word embeddings. These embeddings were trained by researchers at Stanford on 6 billion tokens from Wikipedia entries.

Note: The embeddings .zip file is very large. You may have to increase your global timeout setting to download, ex: options(timeout=100)


5.  Test them out with the canonical word math equation on the GloVe embeddings: "berlin" - "germany" + "france" = ?

Hint: you'll need to convert the GloVe dataframe to a matrix and set the row names in order to use our synonym function.

6.  Recreate parts 2 and 3 above using the the GloVe embeddings in place of the ones you made. How do they compare? What are the implications for applications of these embeddings?
